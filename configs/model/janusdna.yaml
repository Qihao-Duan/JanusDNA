# Use open-source version of Mamba
_name_: janusdna_lm
config:
  _target_: janusdna.configuration_janusdna.JanusDNAConfig
  # From original MambaConfig
  hidden_size: 256
  flex_attn_n_embd: 256
  num_hidden_layers: 15
  vocab_size: 12
  ssm_cfg:
    d_state: 16
    d_conv: 4
    expand: 2
    dt_rank: "auto"
    dt_min: 0.001
    dt_max: 0.1
    dt_init: "random"
    dt_scale: 1.0
    dt_init_floor: 1e-4
    conv_bias: true
    bias: false
    use_fast_path: true
  # rms_norm: true
  # fused_add_norm: true
  # residual_in_fp32: false
  # pad_vocab_size_multiple: 8
  # Not in original MambaConfig, but default arg in create_block in mamba_ssm repo; used in layer norm
  rms_norm_eps: 1e-6

  # Used in init_weights
  # initializer_cfg:
  initializer_range: 0.02
    # rescale_prenorm_residual: true
    # n_residuals_per_layer: 1

  # Caduceus-specific params
  bidirectional: true,
  bidirectional_strategy: "add"
  bidirectional_weight_tie: true
  # rcps: false

  # Used for RCPSEmbedding / RCPSLMHead (will be filled in during model instantiation using info from tokenizer)
  # complement_map: null

  # added specifically for caduceus_ffn_moe_attn
  # output_hidden_states: false
  # return_dict: true

  # # expert
  # expert_layer_period: 4
  # expert_layer_offset: 2
  # num_experts: 4
  # intermediate_factor: 4
  # # attn
  # num_attention_heads: 4
  # is_causal: False
  # attention_dropout: 0.0

  # attn_layer_period: 4
  # attn_layer_offset: 100
  return_dict: true

  
  # moe
  num_experts: 16
  num_experts_per_tok: 2
  expert_layer_period: 4 # 100 means no moe
  expert_layer_offset: 2 # layer_name % expert_layer_period == expert_layer_offset, would be a MOE layer
  output_hidden_states: False
  intermediate_factor: 4

  # bidirectional: True
  # # bidirectional_strategy: "add" # todo: # should be "add", "concat", "ew_multiply" and "final layer transformer"
  # bidirectional_weight_tie: False
  # key params for autoregressive training diagram
  layer_fusion: False # if layer_fusion, bi-directional output of each layer would be fused. If not, will just concat and fuse at last decode layer.

  # attn
  num_attention_heads: 4
  attn_implementation: "flash_attention_2" ## acutally is flex attention
  attn_layer_period: 4 # every 8 layers is an jamba block
  attn_layer_offset: 2 # every 4 layers, there is an attention layer
  
  # final
  final_attention: true
  mid_single_direction_attention: true
  layer_fusion_strategy: "pool" # "pool" or "None"
  final_attention_class: "flex_attention"
  bidirectional_attn_tie: false

  gradient_checkpointing: true